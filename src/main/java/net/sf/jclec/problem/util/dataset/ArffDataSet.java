package net.sf.jclec.problem.util.dataset;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.Reader;
import java.util.ArrayList;
import java.util.List;
import java.util.StringTokenizer;

import net.sf.jclec.problem.classification.classic.ClassicClassificationMetadata;
import net.sf.jclec.problem.classification.classic.ClassicInstance;
import net.sf.jclec.problem.util.dataset.attribute.AttributeType;
import net.sf.jclec.problem.util.dataset.attribute.CategoricalAttribute;
import net.sf.jclec.problem.util.dataset.attribute.IAttribute;
import net.sf.jclec.problem.util.dataset.attribute.NumericalAttribute;
import net.sf.jclec.util.range.Closure;
import net.sf.jclec.util.range.Interval;

/**
 * ArffDataset implementation (Weka dataset)
 * 
 * @author Alberto Cano 
 * @author Amelia Zafra
 * @author Sebastian Ventura
 * @author Jose M. Luna 
 * @author Juan Luis Olmo
 */

public class ArffDataSet extends AbstractDataset
{
	/////////////////////////////////////////////////////////////////
	// --------------------------------------- Serialization constant
	/////////////////////////////////////////////////////////////////

	/** Generated by Eclipse */

	private static final long serialVersionUID = 1L;

	/////////////////////////////////////////////////////////////////
	// ------------------------------------------- Internal Variables
	/////////////////////////////////////////////////////////////////

	/** The keyword used to denote the relation name */

	static String ARFF_RELATION = "@relation";

	/** The keyword used to denote the attribute description */

	static String ARFF_ATTRIBUTE = "@attribute";

	/** The keyword used to denote the start of the arff data section */

	static String ARFF_DATA = "@data";

	/** Symbol which represents missed values */

	protected String missedValue;

	/** Symbol which represents commentted values */

	protected String commentedValue;

	/** Symbol which represents the separation between values */

	protected String separationValue;

	/////////////////////////////////////////////////////////////////
	// -------------------------------------------------- Constructor
	/////////////////////////////////////////////////////////////////

	public ArffDataSet()
	{
		super();

		missedValue = "?";
		separationValue = ",";
		commentedValue = "%";
	}

	/////////////////////////////////////////////////////////////////
	// ----------------------------------------------- Public methods
	/////////////////////////////////////////////////////////////////

	/**
	 * Load examples from the data set
	 * 
	 * Open dataset, obtain metadata, read examples, close dataset
	 */

	public void loadExamples()
	{	
		File datasetFile = new File(fileName);
		Reader fileReader;

		examples = new ArrayList<IExample>();

		try {
			// Open dataset
			fileReader = new BufferedReader(new FileReader(datasetFile));

			// Read the dataset
			readLoop: do
			{
				StringTokenizer tokenizer;
				String line = "", str = "";

				// Read next line
				while(line.startsWith(commentedValue) || line.equalsIgnoreCase(""))
				{
					line = ((BufferedReader) fileReader).readLine();

					if(line == null)
					{
						// dntln("The dataset specification xml have not been defined");
						break readLoop;
					}
				}

				// Fix attribute information
				if(line.startsWith(ARFF_ATTRIBUTE))
				{
					int index = line.indexOf('[');
					if(index != -1)
						line = line.substring(0,index) + " [" + line.substring(index+1);

					index = line.indexOf('{');
					if(index != -1)
						line = line.substring(0,index) + " {" + line.substring(index+1);
				}

				// Tokenizer the line
				tokenizer = new StringTokenizer(line);
				str = tokenizer.nextToken();

				// Proccess the dataset name	
				if(str.equalsIgnoreCase(ARFF_RELATION))
				{
					String name = tokenizer.nextToken();
					setName(name);
					continue;
				}

				// Process data
				if(str.equalsIgnoreCase(ARFF_DATA))
				{
					continue;
				}

				// Process @end bag
				if(str.equalsIgnoreCase("@end"))
				{
					continue;
				}

				// Proccess the attribue	
				if(str.equalsIgnoreCase(ARFF_ATTRIBUTE))
				{
					String name = tokenizer.nextToken();
					String type = tokenizer.nextToken();

					if(type.equalsIgnoreCase("REAL") || type.equalsIgnoreCase("NUMERIC"))
					{
						addAttributeToSpecification(type, line, name);
					}
					else if(type.equalsIgnoreCase("RELATIONAL"))
					{
						// Ignore and read next the relational attributes
					}
					else
						addAttributeToSpecification("STRING", line, name);

					continue;
				}

				tokenizer = new StringTokenizer(line, separationValue);

				// Instance values
				double[] values = new double[metadata.numberOfAttributes()];

				// Parse each attribute value
				for(int i = 0; i < metadata.numberOfAttributes(); i++)
				{
					IAttribute attribute = metadata.getAttribute(i);
					values[i] = attribute.parse(tokenizer.nextToken().trim());
				}

				if(((ClassicClassificationMetadata) metadata).getClassIndex() == -1)
					((ClassicClassificationMetadata) metadata).setClassIndex(metadata.numberOfAttributes());

				// Parse class value
				IAttribute attribute = ((ClassicClassificationMetadata) metadata).getClassAttribute();
				double classValue = attribute.parse(tokenizer.nextToken().trim());

				// Create instance
				ClassicInstance instance = new ClassicInstance(metadata.numberOfAttributes());
				instance.setValues(values);
				instance.setClassValue(classValue);

				//Add the instance to the dataset
				examples.add(instance);
			}
			while (true);

			fileReader.close();

		} catch (FileNotFoundException e) {
			e.printStackTrace();
		} catch (Exception e){
			e.printStackTrace();
		}

		computeAttributeDomains();
	}

	/**
	 * Set the dataset examples
	 * 
	 * @param examples the examples
	 */

	public void setExamples(ArrayList<IExample> examples)
	{
		this.examples = examples;
	}

	/**
	 * Get the dataset examples
	 * 
	 * @return examples
	 */

	public ArrayList<IExample> getExamples()
	{
		return examples;
	}

	/**
	 * Add the new examples to the dataset
	 * 
	 * @param newexamples examples to add
	 */

	public void addExamples(ArrayList<IExample> newexamples)
	{
		this.examples.addAll(newexamples);
	}

	/////////////////////////////////////////////////////////////////
	// ---------------------------------------------- Private Methods
	/////////////////////////////////////////////////////////////////

	/**
	 * Add new attribute to the dataset specification
	 * 
	 * @param type Attribute type
	 * @param interval Intervals value
	 * @param name Attribute name
	 */
	private void addAttributeToSpecification(String type, String interval, String name)
	{
		// If the attribute is numerical
		if(type.equalsIgnoreCase("REAL") || type.equalsIgnoreCase("NUMERIC"))
		{
			NumericalAttribute attribute = new NumericalAttribute();
			attribute.setName(name);

			Interval intervals = new Interval();

			intervals.setClosure(Closure.ClosedClosed);

			attribute.setInterval(intervals);

			//Add new attribute to the specification
			metadata.addAttribute(attribute);
		}
		else
		{
			//Obtain the categorical values
			int minIndex = interval.indexOf("{");
			int maxIndex = interval.indexOf("}");

			interval = interval.substring(minIndex+1, maxIndex);

			if(minIndex < maxIndex){
				CategoricalAttribute attribute = new CategoricalAttribute();
				attribute.setName(name);

				StringTokenizer categories = new StringTokenizer(interval, ",");
				List<String> categoriesList = new ArrayList<String>();

				while(categories.hasMoreTokens())
					categoriesList.add(categories.nextToken().trim());

				attribute.setCategories(categoriesList);

				//Add new attribute to the specification
				metadata.addAttribute(attribute);
			}
		}
	}

	/**
	 * Compute attribute domains
	 */

	private void computeAttributeDomains()
	{
		double[] minValues = new double[metadata.numberOfAttributes()];
		double[] maxValues = new double[metadata.numberOfAttributes()];

		for(int i = 0; i < metadata.numberOfAttributes(); i++)
		{
			minValues[i] = Double.MAX_VALUE;
			maxValues[i] = -Double.MAX_VALUE;
		}

		for(IExample instance : examples)
		{
			for(int i = 0; i < metadata.numberOfAttributes(); i++)
			{
				double value = instance.getValue(i);

				if(value < minValues[i])
					minValues[i] = value;
				if(value > maxValues[i])
					maxValues[i] = value;
			}
		}

		for(int i = 0; i < metadata.numberOfAttributes(); i++)
			if(metadata.getAttribute(i).getType() == AttributeType.Numerical)
			{
				Interval interval = new Interval(minValues[i], maxValues[i], Closure.ClosedClosed);
				((NumericalAttribute) metadata.getAttribute(i)).setInterval(interval);
			}
	}

	/**
	 * Copy method
	 * 
	 * @return A copy of this dataset
	 */

	@Override
	public IDataset copy() {

		ArffDataSet dataset = new ArffDataSet();

		dataset.setName(datasetName);
		dataset.setMetadata(metadata.copy());

		ArrayList<IExample> examples = new ArrayList<IExample>();

		for(IExample example : this.examples)
			examples.add(example.copy());

		dataset.setExamples(examples);

		return dataset;
	}
}