package net.sf.jclec.problem.util.dataset;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.Reader;
import java.util.ArrayList;
import java.util.List;
import java.util.StringTokenizer;

import net.sf.jclec.problem.classification.classic.ClassicClassificationMetadata;
import net.sf.jclec.problem.classification.classic.ClassicInstance;
import net.sf.jclec.problem.util.dataset.attribute.CategoricalAttribute;
import net.sf.jclec.problem.util.dataset.attribute.IAttribute;
import net.sf.jclec.problem.util.dataset.attribute.IntegerAttribute;
import net.sf.jclec.problem.util.dataset.attribute.NumericalAttribute;
import net.sf.jclec.util.range.Closure;
import net.sf.jclec.util.range.Interval;

/**
 * KeelDataSet implementation
 * 
 * @author Alberto Cano 
 * @author Amelia Zafra
 * @author Sebastian Ventura
 * @author Jose M. Luna 
 * @author Juan Luis Olmo
 */

public class KeelDataSet extends AbstractDataset
{
	/////////////////////////////////////////////////////////////////
	// --------------------------------------- Serialization constant
	/////////////////////////////////////////////////////////////////

	/** Generated by Eclipse */

	private static final long serialVersionUID = 1L;

	/////////////////////////////////////////////////////////////////
	// ------------------------------------------- Internal Variables
	/////////////////////////////////////////////////////////////////

	/** The keyword used to denote the relation name */

	static String KEEL_RELATION = "@relation";

	/** The keyword used to denote the attribute description */

	static String KEEL_ATTRIBUTE = "@attribute";

	/** The keyword used to denote the start of the arff data section */

	static String KEEL_DATA = "@data";

	/** The keyword used to denote input attributes */

	static String KEEL_INPUT = "@inputs";

	/** The keyword used to denote the output attribute */

	static String KEEL_OUTPUT = "@output";

	/** The keyword used to denote the multi-label output class */

	static String KEEL_OUTPUTS = "@outputs";

	/** Symbol which represents missed values */

	protected String missedValue;

	/** Symbol which represents commentted values */

	protected String commentedValue;

	/** Symbol which represents the separation between values */

	protected String separationValue;

	/////////////////////////////////////////////////////////////////
	// -------------------------------------------------- Constructor
	/////////////////////////////////////////////////////////////////

	public KeelDataSet()
	{
		super();

		missedValue = "?";
		separationValue = ",";
		commentedValue = "%";
	}

	/////////////////////////////////////////////////////////////////
	// ----------------------------------------------- Public methods
	/////////////////////////////////////////////////////////////////

	/**
	 * Load examples from the data set
	 * 
	 * Open dataset, obtain metadata, read examples, close dataset
	 */

	public void loadExamples()
	{	
		File datasetFile = new File(fileName);
		Reader fileReader;

		examples = new ArrayList<IExample>();

		try {
			// Open dataset
			fileReader = new BufferedReader(new FileReader(datasetFile));

			// Read the dataset
			readLoop: do
			{
				StringTokenizer tokenizer;
				String line = "", str = "";

				// Read next line
				while(line.startsWith(commentedValue) || line.equalsIgnoreCase(""))
				{
					line = ((BufferedReader) fileReader).readLine();

					if(line == null)
					{
						// Check if all the metadata information have been loaded
						break readLoop;
					}
				}

				// Fix attribute information
				if(line.startsWith(KEEL_ATTRIBUTE))
				{
					int index = line.indexOf('[');
					if(index != -1)
						line = line.substring(0,index) + " [" + line.substring(index+1);

					index = line.indexOf('{');
					if(index != -1)
						line = line.substring(0,index) + " {" + line.substring(index+1);
				}

				// Tokenizer the line
				tokenizer = new StringTokenizer(line);
				str = tokenizer.nextToken();

				// Proccess the dataset name	
				if(str.equalsIgnoreCase(KEEL_RELATION))
				{
					String name = tokenizer.nextToken();
					setName(name);
					continue;
				}

				// Process data
				{
					if(str.equalsIgnoreCase(KEEL_DATA))
						continue;
				}

				// Proccess the attribue	
				if(str.equalsIgnoreCase(KEEL_ATTRIBUTE))
				{
					String name = tokenizer.nextToken();
					String type = tokenizer.nextToken();

					if(type.equalsIgnoreCase("REAL") || type.equalsIgnoreCase("INTEGER"))
						addAttributeToSpecification(type, line, name);
					else
						addAttributeToSpecification("STRING", line, name);

					continue;
				}

				// Process input attributes
				if(str.equalsIgnoreCase(KEEL_INPUT))
				{
					continue;
				}

				// Process output class(es)
				if(str.equalsIgnoreCase(KEEL_OUTPUT) || str.equalsIgnoreCase(KEEL_OUTPUTS))
				{
					StringTokenizer tokenizer2 = new StringTokenizer(line, separationValue);

					if(tokenizer2.countTokens() != 1)
					{
					}
					else
					{
						String attributeClass = tokenizer.nextToken();

						if(metadata instanceof ClassicClassificationMetadata)
							((ClassicClassificationMetadata) metadata).setClassIndex(metadata.getAttributeIndex(attributeClass));
					}

					continue;
				}

				tokenizer = new StringTokenizer(line, separationValue);

				double[] values = new double[metadata.numberOfAttributes()];

				// Parse each attribute value
				for(int i = 0; i < metadata.numberOfAttributes(); i++)
				{
					IAttribute attribute = metadata.getAttribute(i);
					values[i] = attribute.parse(tokenizer.nextToken().trim());
				}

				if(((ClassicClassificationMetadata) metadata).getClassIndex() == -1)
				{
					((ClassicClassificationMetadata) metadata).setClassIndex(((ClassicClassificationMetadata) metadata).numberOfAttributes());
				}

				// Parse class value
				IAttribute attribute = ((ClassicClassificationMetadata) metadata).getClassAttribute();
				double classValue = attribute.parse(tokenizer.nextToken().trim());

				// Create instance
				ClassicInstance instance = new ClassicInstance(metadata.numberOfAttributes());
				instance.setValues(values);
				instance.setClassValue(classValue);

				//Add the instance to the dataset
				examples.add(instance);
			}
			while (true);

			fileReader.close();

		} catch (FileNotFoundException e) {
			e.printStackTrace();
		} catch (Exception e){
			e.printStackTrace();
		}
	}

	/**
	 * Set the dataset examples
	 * 
	 * @param examples the examples
	 */

	public void setExamples(ArrayList<IExample> examples)
	{
		this.examples = examples;
	}

	/**
	 * Get the dataset examples
	 * 
	 * @return examples
	 */

	public ArrayList<IExample> getExamples()
	{
		return examples;
	}

	/**
	 * Add the new examples to the dataset
	 * 
	 * @param newexamples examples to add
	 */

	public void addExamples(ArrayList<IExample> newexamples)
	{
		this.examples.addAll(newexamples);
	}

	/////////////////////////////////////////////////////////////////
	// ---------------------------------------------- Private Methods
	/////////////////////////////////////////////////////////////////

	/**
	 * Add new attribute to the dataset specification
	 * 
	 * @param type Attribute type
	 * @param interval Intervals value
	 * @param name Attribute name
	 */
	private void addAttributeToSpecification(String type, String interval, String name)
	{
		// If the attribute is numerical
		if(type.equalsIgnoreCase("REAL"))
		{
			NumericalAttribute attribute = new NumericalAttribute();
			attribute.setName(name);

			// Obtain the intervals
			int minIndex = interval.indexOf("[");
			int maxIndex = interval.indexOf("]");

			interval = interval.substring(minIndex+1, maxIndex);

			if(minIndex < maxIndex){

				StringTokenizer tkInterval = new StringTokenizer(interval, ",");

				Interval intervals = new Interval();

				intervals.setClosure(Closure.ClosedClosed);
				intervals.setLeft(Double.valueOf((String) tkInterval.nextElement()));
				intervals.setRight(Double.valueOf((String) tkInterval.nextToken()));

				attribute.setInterval(intervals);

				//Add new attribute to the specification
				metadata.addAttribute(attribute);
			}
		}
		else if(type.equalsIgnoreCase("INTEGER"))
		{			
			IntegerAttribute attribute = new IntegerAttribute();
			attribute.setName(name);

			// Obtain the intervals
			int minIndex = interval.indexOf("[");
			int maxIndex = interval.indexOf("]");

			interval = interval.substring(minIndex+1, maxIndex);

			if(minIndex < maxIndex){

				StringTokenizer tkInterval = new StringTokenizer(interval, ",");

				net.sf.jclec.util.intset.Interval intervals = new net.sf.jclec.util.intset.Interval();

				intervals.setClosure(net.sf.jclec.util.intset.Closure.ClosedClosed);
				intervals.setLeft(Integer.valueOf((String) tkInterval.nextElement()));
				intervals.setRight(Integer.valueOf((String) tkInterval.nextToken().trim()));

				attribute.setInterval(intervals);

				//Add new attribute to the specification
				metadata.addAttribute(attribute);
			}
		}
		else
		{
			//Obtain the categorical values
			int minIndex = interval.indexOf("{");
			int maxIndex = interval.indexOf("}");

			interval = interval.substring(minIndex+1, maxIndex);

			if(minIndex < maxIndex){
				CategoricalAttribute attribute = new CategoricalAttribute();
				attribute.setName(name);

				StringTokenizer categories = new StringTokenizer(interval, ",");
				List<String> categoriesList = new ArrayList<String>();

				while(categories.hasMoreTokens())
					categoriesList.add(categories.nextToken().trim());

				attribute.setCategories(categoriesList);

				//Add new attribute to the specification
				metadata.addAttribute(attribute);
			}
		}
	}

	/**
	 * Copy method
	 * 
	 * @return A copy of this dataset
	 */

	@Override
	public IDataset copy() {

		KeelDataSet dataset = new KeelDataSet();

		dataset.setName(datasetName);
		dataset.setMetadata(metadata.copy());

		ArrayList<IExample> examples = new ArrayList<IExample>();

		for(IExample example : this.examples)
			examples.add(example.copy());

		dataset.setExamples(examples);

		return dataset;
	}
}